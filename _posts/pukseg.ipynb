txt = open("/home/chen/Downloads/DATA20231211/微信互联网平民创业1.txt", "r", encoding='utf-8').read()

# Ddfine input and output file paths
input_file_path = txt
output_file_path = r"/home/chen/Downloads/DATA20231211/微信互联网平民创业1seg.txt"

# Use jieba.posseg to get words and their POS tags
seg = pkuseg.pkuseg(postag=True)
words = seg.cut(txt)

# Separate dictionaries for nouns, adjectives ,and verbs
noun_counts = {}
adj_counts = {}
verb_counts = {}

# Start the timer
start_time = time.time()

for word, pos in words:
    if len(word) == 1:
        continue
    elif pos == 'n':
        noun_counts[word] = noun_counts.get(word, 0) + 1
    elif pos == 'a':
        adj_counts[word] = adj_counts.get(word, 0) + 1
    elif pos == 'v':
        verb_counts[word] = verb_counts.get(word,0) + 1
        
# Sort the counts for each POS in descending order
sorted_nouns = sorted(noun_counts.items(), key=lambda item: item[1], reverse=True)
sorted_adjectives = sorted(adj_counts.items(), key=lambda item:item[1], reverse=True)
sorted_verbs = sorted(verb_counts.items(), key=lambda item:item[1], reverse=True)

# Save the sorted results to a single text file
with open(output_file_path, "w", encoding='utf-8' ) as f:
    f.write("Nouns:\n")
    for word, count in sorted_nouns:
        f.write(f"{word}: {count}\n")
    f.write("\nAdjectives:\n")
    for word, count in sorted_adjectives:
        f.write(f"{word}: {count}\n")
    f.write("\nVerbs:\n")
    for word, count in sorted_verbs:
        f.write(f"{word}: {count}\n")
                
# End the timer
end_time = time.time()
                
time_taken = end_time - start_time
                
print(f"Time taken to execute the code: {time_taken:.2f} seconds")
